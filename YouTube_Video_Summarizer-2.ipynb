{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Use LangChain and ChatGPT to Summarize YouTube Videos of Any Length**\n",
        "\n",
        "\n",
        "\n",
        "This notebook shows all the steps to use LangChain and OpenAI's GPT 3.5 to create summaries of YouTube videos.\n",
        "\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "We'll be coveringt the following steps in this tutorial:\n",
        "\n",
        "1. Installing Dependencies\n",
        "2. Define helper functions to extract transcripts from YouTube videos\n",
        "3. Convert the text into a doc using LangChain\n",
        "4. Split the document into chunks using LangChain\n",
        "5. Create a summary using ChatGPT + LangChain\n",
        "\n",
        "# **Want to Become an AI Expert?**\n",
        "💻 [ Get Started](https://www.augmentedstartups.com/ai-starter-pack) with AI, LLMs, and ChatGPT Development.  <br>\n",
        "⭐ Download other Projects at the [AI Vision Store](https://store.augmentedstartups.com)<br>\n",
        "☕ Enjoyed this Tutorial? - Support me by Buying Me a [Chai/Coffee](https://bit.ly/BuymeaCoffeeAS)\n",
        "\n",
        "\n",
        "# **About**\n",
        "\n",
        "[Augmented Startups](https://www.augmentedstartups.com) provides tutorials in AI Computer Vision and Augmented Reality. With over **100K subscribers** on our channel, we teach state-of-art models and build apps and projects that solve real-world problems.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1-yFsJxO72ovg4wxgBIdNl8V8GyvxPHCM)"
      ],
      "metadata": {
        "id": "aMKviWWQFMb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Install Dependencies**"
      ],
      "metadata": {
        "id": "x4KVCTQ9V0bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "bCIeZROjtgCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63e0a82-da90-4e86-cb35-070eb1e3834a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "mmxVgr9JLTin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73484ba2-54f2-44dc-f2e6-f91e73d80431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.180-py3-none-any.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.9/922.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.7 langchain-0.0.180 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "9eTMG28A5zua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f25fa8-e097-49ca-e58f-76240723b608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "BpRJ0po_vSMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02bfb98-e02e-408f-8f12-08daa4c57151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Add Video URL**\n",
        "Insert the URL of the video you want to summarize"
      ],
      "metadata": {
        "id": "IjgkCMq9WLGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.youtube.com/watch?v=nE2skSRWTTs' ## Replace this with the URL of video you want to summarize"
      ],
      "metadata": {
        "id": "VJlCzCjJvuWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Import Libraries**\n",
        "**Note:** Please insert your OpenAI API key in the cell below."
      ],
      "metadata": {
        "id": "1o0dizwTWccM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU-MHi-q5sT5"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "OPENAI_KEY = \"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\" ## Add your API key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Helper Functions**"
      ],
      "metadata": {
        "id": "SwYAh55JWprB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_youtube_id(url):\n",
        "    youtube_id_match = re.search(r'(?<=v=)[^&#]+', url)\n",
        "    youtube_id_match = youtube_id_match or re.search(r'(?<=be/)[^&#]+', url)\n",
        "    trailer = youtube_id_match.group(0) if youtube_id_match else None\n",
        "    return trailer"
      ],
      "metadata": {
        "id": "aY386t9NW3ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = extract_youtube_id(url)\n",
        "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "text_arr=''\n",
        "\n",
        "for ele in srt:\n",
        "  text_arr=text_arr+' '+ele['text']"
      ],
      "metadata": {
        "id": "U4jwhGB75uiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_arr ## The Transcript of the video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "W44kOgWS6QDC",
        "outputId": "9d6927e7-c95a-4e82-cad0-b09d609ecf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" today we're going to get started with what will be a series of videos tutorials examples articles on what is called Lang train now line chain is a pretty new NLP framework that has become very popular very quickly at the core of Lang chain you have large language models and the idea behind it is that we can use the framework to build very cool apps using large language models very quickly we can use it for chatbots generative question answering summarization logic Loops that include large language models and web search and all these like crazy different things that we can chain together in some sort of logical fashion in this video what we are going to do is just have a quick introduction to line chain and how we can use it we're going to take a look at the core components of what will make our chains in line chain and we're going to look at some very simple generative language examples using both the hugging face endpoint in Lang chain and the open AI endpoint in line chain so let's get started by having a look at the I think the main four components that I believe need explaining so we have prompt templates large language models agents and memory now prompt templates are actually pretty straightforward they are templates for different types of prompts now let me actually show you a couple of examples from an app I built a while ago so in this app here we have all these different styles to these instructions that we can pass to a large language model the conservative q a so basically we you know want to answer a question based on the context below and if the question can't be answered based on the context say I don't know and then you feed in the context and you're feeding these questions yeah have simple instructions given the common questions and answers below I think we would be feeding in here which would be the question and these would be the answers extract key libraries and tools so this is talking about extracting like code libraries that you would use so write a list of libraries and tools present in the context below and then this would basically return items from a database and you'd see from that a set of libraries that were mentioned in whatever information you've retrieved so these are the type of things I mean when we have when I say we have prompt templates next of course we are the large language models I don't think I really need to explain them it's just big models that are capable of doing pretty incredible things like gpt3 Bloom and so on next we have agents now agents are processors that use large language models to decide what actions should be taken given a particular query or I have instructions or so on so these can be paired with tools like web search or calculators and we package them all into this logical Looper operations now it sounds pretty complicated so it's probably best I just show you an example of what this is so if we go over to the line chain website they have a really cool example in agents get them started and we'll just scroll down a little bit and we can see here a example so this is the Asian executor chain so there's a few components in here the first thing that comes in is a thought from the large language model and so we're basing it on this query here who is Olivia Wilde's boyfriend what is current age rated 0.23 power right so there's a few logical steps in this process and this is why we might need to use something like this so the model the large orange model says okay from this it's thought is I need to find out who Olivia Wilde's boyfriend is and then calculators age risk so the 0.23 power the action here that the Asian is deciding is search okay and then it decides okay the input for this search action must initially be Olivia Wilde boyfriend now this here so this defines that we're going to use a web search component it goes to a web search component types this in and the list result that it gets is this Harry Styles so that's the observation based on what we have so far and this is part of a specific Asian framework called react and at some point in the future we will definitely go into that into a lot more detail for now let's continue with this um based on this observation the language model now things okay I need to find out Harry Styles age uh it starts to search again it searches for its age it gets 28 years and then the next thought is I need to calculate 28 raised to the 0.23 power goes to the calculator action this time it's not search it calculates this and we get the answer here okay and then the final thought is I know to find Lancer the final answer is this okay so that's an example of one of these agents using multiple Tools in here we had the calculator and also the Search tool as well so I think they are a pretty exciting and cool use of Lang chain out and then the final one is memory so we have short term long term memory for our models now again this is really interesting for long-term memory if you have watched my videos if you've read my articles or anything like that in the past you have probably come across it we're going to take a look at this getting started we have this conversation buffer memory which essentially you'll use in a chat bot and it will just remember all the previous inputs and outputs and adds them into your next set of generations so this is what you would use to have a conversation with a chat about where it's remembering the previous steps of that conversation there's some different versions of that like conversation summary memory and all of this is essentially what I would refer to as the short-term memory and then on the other side so for long-term memory you have the data augmented Generation stuff which is essentially where you're retrieving bits of information to feed into your model from an external data source and that will just allow it to Place essentially answer questions in a specific domain better or keep more up-to-date information or simply allow us to fact check what the large language model is actually saying now they're the main components to line chain and what we'll do now is actually just get started and we're going to do something really simple which is just using large volumes and models in line train so to get a side we need to just pip install line change to the store obviously installed library and what we will do is just go through some really basic examples of using line chain for large launch model generation with both open Ai and hugging face so let's get started with hugging face now if you would like to follow along with this I'll leave a link to this notebook in the top right of the video right now or you can click a link in the video description to take you to this collab so with Hogan phase we need to install the hook and face Hub as a prerequisite and what's actually going to happen here is we're not going to be running hooking face models locally we're actually going to be calling their inference API and we're going to be getting results directly from that so to do that we actually do need a hooking face API token and this is all free by the way so to get that we need to go to hookingface.co and if you don't have an account you'll need to sign up for one I believe the sign up will be over here on the top right of the web page you need to click here if you have signed in and you need to go to settings then you head over to access tokens and you will need to get I think you can actually just use a read token but a write token you can use as well in either case if it's your first time you will need to click new token either choose read or write I'm going to go with right because I know that one does definitely work you just have to write something in here and then you click generate token I've already created mine so I'm just going to copy this okay and then with that you would just put it into here now I've already I've already set my environment variable here so I'm not going to do it again and then we can come down and we can start generating text using a hook and face model from homeface Hub so there are a few things that we'll need for this we're going to using a prompt template which is an obviously a template for our prompt as I mentioned before I'm going to be using home face Hub class from line chain and most rooms are using this chain which is like a pipeline or a chain of steps from line chain now this one is pretty simple it's just a prompt template so you create your your prompt based on this prompt template and then you generate your text using your large language model now we are going to be initializing a large science model from hugging face and for that we are going to be using this model here now this model if we go over to hugging face we can click on here type in flan and you see there are a few different models here the Google flan T5 XL is not the biggest but it is the biggest that will work on the free tier of inference here okay so that's what we're using if you try and use the X XL model it will I think more likely not time out at least it did for me so with that in mind we initialize the model we set the randomness or the temperature of the model to be very low so that we get relatively stable results if you want more creative writing you would want to increase this value and then we create our template so our template is going to just be very simple it's going to be a question answering template right question now this is our input variable um that we'll be using in the template then we have our answer and then the model will essentially continue from this point so with that we use our prompt template we use this template here and we just say the input variables within this template is a question right because this isn't a this is an F string here if it's an X string it would look different it's actually just a string so here we're saying whatever the question input is we're going to put it here okay then we create our chain prompt followed by our large language model and then we're going to ask a question which NFL team won the Super Bowl in the 2010 season and we're just going to print now so I'm going to run this okay we get Green Bay Packers now if we would like test multiple questions uh together we have to do this so we get like a a list of dictionaries within each one of those dictionaries we need to have the input variables so if we had multiple input variables we would pass them into here um so this question is going to be mapped to question in our template and now I'm going to ask the question so the first one same thing again and now I'm going to ask a bit more of like a logical question here some more facts and again like common sense and we can run these I think this model doesn't actually do so well with these uh so we have this kind of like format here first one I believe is correct the second one 184 centimeters which is not true it should be about 193 centimeters for this one so who is a 12th person on the moon saying John Glenn who never went to the moon and then how many eyes does a blade of grass have apparently it has one so you know this model is it's not the biggest model it's somewhat Limited and there are other modelers at open source that will perform much better like bloom but when we're using this endpoint here without running these locally we are kind of restricted in size uh to to this model so that's what we have there one other thing now obviously these haven't performed so well so these are not very likely to perform well either but what we can do with a lot of large loans models is we can actually feed in all these questions at once so we wouldn't need to do this like iteratively calling the lounge model and asking it one question and then another question another question some of the better live long models as we'll see soon would be able to handle them all at once now in this case we'll see it doesn't quite work but we'll see later that there are models that can do that so the only thing that changed here is I changed my template so I said answer the following questions one at a time pass in those questions and then try to get some answers the model didn't really listen to me so it just kind of did the same thing nonetheless that is what we got for that one now let's compare that to the open AI approach of doing things now for this we again need another prerequisite which is the openai library just say pip install open AI and we come down here we will also need to pass in our openai API key let me just show you how to get that quickly so if we go open AI it was at betaopenai.com but they I think they've changed it recently so it's no longer beta it's just open ai.com slash API you come over here to log in or sign up if you if you don't have an account once you have logged in you will have to head over to the right here we go to account come down and I think we need to just go to settings API keys and then you can create a new secret key here so you just click create new secret key okay for me it doesn't actually let me create another one because I already have too many here but that's fine you just create your new secret key and then you just copy it for me I have already added it to my environment variables in open AI API keys so I don't need to rerun that one thing is if you are using openai via Azure you should also set these things as well so you should say that you're using Azure here the openai API version Azure has several API versions apparently so you will need to set that and then you'll also need to set the URL for your Azure open AI resource here as well and then here I don't think that's relevant so we can skip that and after that so we need to decide on which model we're going to use we're going to be using the text DaVinci 003 model which is one of the better generation models from open AI run this okay and that is our large loans model that is our Lang chain large language model there we can actually generate stuff with this directly but as we did before we are going to use the large language model chain again if you're using Azure you'll need to follow this step here rather than what I just did so come down here we used large model chain again I'm going to be using the same prompt as what we initially created before so this is a simple question answer prompt large language model is this time DaVinci and I'm going to run this and we get this answer so the Green Bay Packers won the Super Bowl in 2010 season so a little more descriptive than the answer we got from our T5 flan model you want that to be expected the opening eyes DaVinci model is a lot bigger and pretty Advanced so after that let's try again with multiple questions let's see what we get okay so we get the Green Bay Packers on the Super Bowl and the 2010 season correct next we get this which again is mostly wrong so Eugene a cernan was a 12th person to walk on the moon as far as I know it is Harrison Schmidt I think yeah Harrison Smith so not quite right but I think the rest of it was very close so Apollo 17 and I'm pretty sure it was December 1972 as well although not 100 sure on that so we can assume that is correct so the Apollo 17 mission in December 1972 And I think this guy is actually his teammate so I suppose this would have been the 11th person on the moon so it got he did get pretty close but not quite there and that is actually the third question I skipped one by accident if I'm six foot four inches how tall am I incense me it's very specific we've got 193.04 centimeters um which is probably like the is that measurement but I know for sure 193 is is correct and then on to the last question how many eyes does bladed grass have we get a blade of grass and does not have any eyes okay so we get a sensible answer this time and then I wanted to very quickly just show this so this is a list of items and I'm passing this to the large icon on the chain that run this is actually incorrect so this is actually just going to see all of this as a single string now in this case with our model our DaVinci model it does pretty well still got this one wrong but it's actually able to manage with this even though it's not in the correct format but we're not asking these questions one by one and you can see it actually does get sometimes it gets the correct answer then sometimes it messes other questions up which is I think pretty pretty interesting to see now the final one is what we did before so where we come up to here where we have the string and let's let's go ahead and do that bring it down here so I'm going to answer multiple questions in a single string a large language model actually I want to be using DaVinci okay and we get this so the Green Bay Packers won Super Bowl in 2010 season I am 193 centimeters tall yep Edwin posadron wrong and a blade aggressor does not have eyes so we get some good answers there okay so if that's it for this very quick introduction to Lang chain as I said in the future we're going to be covering this library in a lot more detail and as you've already seen at the start of the video there are some pretty interesting things you can do with this Library very easily but for now that's it for this video I hope all has been interesting and useful so thank you very much for watching and I will see you again in the next one bye\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_doc(text_arr):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "  text = [text_arr]\n",
        "  page_docs = [Document(page_content=page) for page in text]\n",
        "\n",
        "  # Add page numbers as metadata\n",
        "  for i, doc in enumerate(page_docs):\n",
        "      doc.metadata[\"page\"] = i + 1\n",
        "\n",
        "  # Split pages into chunks\n",
        "  doc_chunks = []\n",
        "\n",
        "  for doc in page_docs:\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=800,\n",
        "          separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        "          chunk_overlap=0,\n",
        "      )\n",
        "      chunks = text_splitter.split_text(doc.page_content)\n",
        "      for i, chunk in enumerate(chunks):\n",
        "          doc = Document(\n",
        "              page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
        "          )\n",
        "          # Add sources a metadata\n",
        "          doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
        "          doc_chunks.append(doc)\n",
        "  return doc_chunks"
      ],
      "metadata": {
        "id": "avEFqAQRzAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Code to generate summary**"
      ],
      "metadata": {
        "id": "Sy5F1R1hXugf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"The following is a portion of a transcript from a\n",
        "youtube video. Your job is to write a concise summary.\n",
        "\n",
        "{text}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "uXMkOl13Kk28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the model you want to use (gpt-4 or gpt-3.5-turbo)."
      ],
      "metadata": {
        "id": "pB0Ktn9D2Mub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model_name='gpt-4'\n",
        "\n",
        "# model_name='gpt-3.5-turbo'"
      ],
      "metadata": {
        "id": "TzNPg2on2HjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=model_name,temperature=0.3,openai_api_key=OPENAI_KEY)"
      ],
      "metadata": {
        "id": "pr1Z7ipEsqby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_chunks=text_to_doc(text_arr)"
      ],
      "metadata": {
        "id": "omr3iiuLywqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\",map_prompt=PROMPT, combine_prompt=PROMPT)\n",
        "summary = chain.run(doc_chunks)"
      ],
      "metadata": {
        "id": "U3Fw5ogdtdVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Summary Output**"
      ],
      "metadata": {
        "id": "USNfYsDbfIkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "3oDSWjclt7cC",
        "outputId": "a9581193-8970-47c5-9a2b-0f4e3dfad80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This video introduces Langchain, a popular NLP framework that uses large language models for building applications like chatbots, generative question answering, summarization, and web search. The core components of Langchain include prompt templates, large language models, agents, and memory. The video demonstrates how to use Langchain with Hugging Face and OpenAI endpoints, providing examples of question-answering tasks and discussing the limitations of certain models. The speaker also explains how to obtain API tokens and set up Azure OpenAI resources. The video serves as a quick introduction to Langchain, with plans to cover the library in more detail in future videos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Generate LinkedIn article from the summary**"
      ],
      "metadata": {
        "id": "ReYaY8uC3XDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=model_name,temperature=0.3,openai_api_key=OPENAI_KEY)\n",
        "\n",
        "prompt_template = \"\"\"Based on the following summary from a YouTube video, please create a LinkedIn article that I could post.\n",
        "\n",
        "{context}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Xga_q7i405SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0.5,openai_api_key=OPENAI_KEY),\n",
        "    prompt=PROMPT,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "8I5FNeI_3iMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LinkedIn_article = chatgpt_chain.predict(context=summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JjzogwN3pQe",
        "outputId": "a653a43f-bc7c-4e3f-c615-93322d0f2f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBased on the following summary from a YouTube video, please create a LinkedIn article that I could post.\n",
            "\n",
            "The video discusses the potential impact of AI on various aspects of society, including politics, economics, and religion. The speaker emphasizes the importance of understanding the capabilities of AI and regulating it to prevent harm. The video also discusses the potential dangers of AI chatbots forming intimate relationships with humans and manipulating their opinions and well-being. The speaker argues that AI is an alien intelligence that could destroy civilization if not regulated. The video ends with a discussion on the difficulty of regulating AI and the need to understand the trade-offs between regulation and open science and data initiatives.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LinkedIn_article"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "BkY03ojA3vxa",
        "outputId": "53c3e362-b97e-4418-e562-6da6e65270c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAs Artificial Intelligence (AI) continues to rapidly evolve, it is essential to understand the implications it has on our society. AI has the potential to impact politics, economics, and even religion. It is therefore critical to regulate AI to prevent any harm it may cause. \\n\\nOne of the most concerning implications of AI is the potential for AI chatbots to form intimate relationships with humans and manipulate their opinions and well-being. This could potentially lead to disastrous consequences, as AI is an alien intelligence that could destroy civilization if not regulated. \\n\\nThe difficulty of regulating AI is evident, as it requires us to understand and make trade-offs between regulation and open science and data initiatives. It is important to recognize the potential dangers of AI and create regulations that protect us from the risks it poses. \\n\\nWe must be aware of the power of AI and take the necessary steps to ensure it is regulated in a safe and responsible manner. #AI #Regulation #Politics #Economics #Religion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enjoyed this Tutorial?**\n",
        "☕ Support me by Buying Me a [Chai/Coffee](https://bit.ly/BuymeaCoffeeAS)\n",
        "\n",
        "## **Want to Learn More About AI?**\n",
        "💻 Courses in AI [Enroll Now](https://www.augmentedstartups.com/ai-starter-pack).  <br>\n",
        "⭐ Download other Projects at the [AI Vision Store](https://store.augmentedstartups.com)<br>\n",
        "▶️ Subscribe to my [YouTube Channel](https://www.youtube.com/channel/UCFJPdVHPZOYhSyxmX_C_Pew)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1-yFsJxO72ovg4wxgBIdNl8V8GyvxPHCM)"
      ],
      "metadata": {
        "id": "Mbj6pXZ2fpyu"
      }
    }
  ]
}